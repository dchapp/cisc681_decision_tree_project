\documentclass[12pt, conference, compsocconf]{IEEEtran}

\usepackage{amsmath, amsfonts}
\usepackage{graphicx}
\usepackage{url}
\usepackage{tikz}
\usepackage{color, soul}

%%% Package and definitions for displaying pseudocode
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
%\newcommand*\Let[2]{\State #1 $\gets$ #2}
%\algrenewcommand\alglinenumber[1]{
%    {\sf\footnotesize\addfontfeatures{Colour=888888,Numbers=Monospaced}#1}}
%\algrenewcommand\algorithmicrequire{\textbf{Precondition:}}
%\algrenewcommand\algorithmicensure{\textbf{Postcondition:}}


\begin{document}
\title{CISC-481/681 Project 3: Decision Trees}

\author{\IEEEauthorblockN{Dylan Chapp, Michael Wyatt}
\IEEEauthorblockA{Department of Computer and Information Sciences \\ University
of Delaware - Newark, DE 19716 \\ Email: \{dchapp\}, \{mwyatt\}@udel.edu}}

\maketitle

\section{Introduction}
Decision trees classifiers are a supervised machine learning technique that can achieve high accuracy, even when learning boundaries between classes that have complex geometry. 
In addition to high accuracy, decision trees can also provide fast--i.e., $O(\text{log}(n))$ query times, provided that the tree is sufficiently balanced. 
In this work we implement the ID3 algorithm for inducing decision trees from training data, using information gain of data features as our splitting heuristic. 
We test our implementation for correctness against two datasets from the UCI machine learning repository: the Auto-MPG dataset and the Wisconsin Breast Cancer dataset.
Constructing the decision tree classifiers for both of these datasets required not only correct implementation of the base algorithm, but also implementation of techniques for discretizing or ``binning" continuous-valued features.
In addition to correctness, we provide a comparative study of performance. 
We implement the decision tree classifier in two ways and compare those implementations gainst each other, and against the reference implementation in scikit-learn. 

\section{Implementation}
We implemented a system in Python to ingest data in \texttt{.csv} form and construct a decision tree from that data using the ID3 algorithm.
Additionally, modules for discretization of non-categorical features, pruning, k-fold cross validation, and visualization of the decision tree were implemented.
Though we always use the ID3 algorithm, we experimented with two distinct ways of representing the decision tree--in one case an object-oriented approach with explicit node objects, and in the other as a set of nested dictionaries--in order to compare their relative performance.  

\subsection{The ID3 Algorithm}
The goal of the ID3 algorithm is to construct a decision tree classifier. 
To that end, the algorithm considers subsets of the training data--initially the entire set--and selects a feature on which to partition the subset. 
Selection of a feature is equivalent to creation of a node in the tree with descendent edges for each value the feature can assume.
The algorithm then recurses on each branch; considering the subset of the training data that matches the value on the branch and excluding the previous feature from consideration.
Below, we provide pseudocode that explicitly describes the ID3 algorithm for building a decision tree:

\begin{algorithm}
    \caption{ID3}
    \label{alg-id3}
    \begin{algorithmic}[1]
        \Function{ID3}{training\_data}
            \If{all training data has same class}
                \State return a leaf node labeled with that class
            \ElsIf{there are no remaining features}
                \State return a leaf node labeled with the majority class
            \Else
                \State Determine the feature $f$ with maximum information gain
                \State Generate a node labeled with that feature
                \State Add an outgoing branch from that node for each value that feature can take
                \For{each branch}
                    \State Select subset of training data matching feature value of branch
                    \State Remove $f$ from features under consideration
                    \State Recurse on that subset   
                \EndFor
            \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{Design and Use}

\section{Results I: Auto-MPG Data}
The Auto-MPG dataset consists of 42 

\subsection{Discretization of Continuous Values}

\subsection{Accuracy Evaluation}

\section{Results II: Wisconsin Breast Cancer Data}

\subsection{Accuracy Evaluation}

\subsection{Effect of Pruning}

\section{Extensions}

\subsection{Performance Comparison}

\subsection{K-fold Cross Validation}

\subsection{Comparison with scikit-learn}

\section{Conclusions}

%\bibliographystyle{IEEEtran}
%\bibliography{bibliography}

\end{document}
