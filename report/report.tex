\documentclass[12pt, conference, compsocconf]{IEEEtran}

\usepackage{amsmath, amsfonts}
\usepackage{graphicx}
\usepackage{url}
\usepackage{tikz}
\usepackage{color, soul}

%%% Package and definitions for displaying pseudocode
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
%\newcommand*\Let[2]{\State #1 $\gets$ #2}
%\algrenewcommand\alglinenumber[1]{
%    {\sf\footnotesize\addfontfeatures{Colour=888888,Numbers=Monospaced}#1}}
%\algrenewcommand\algorithmicrequire{\textbf{Precondition:}}
%\algrenewcommand\algorithmicensure{\textbf{Postcondition:}}


\begin{document}
\title{CISC-481/681 Project 3: Decision Trees}

\author{\IEEEauthorblockN{Dylan Chapp, Michael Wyatt}
\IEEEauthorblockA{Department of Computer and Information Sciences \\ University
of Delaware - Newark, DE 19716 \\ Email: \{dchapp\}, \{mwyatt\}@udel.edu}}

\maketitle

\section{Introduction}
Decision trees classifiers are a supervised machine learning technique that can achieve high accuracy, even when learning boundaries between classes that have complex geometry. 
In addition to high accuracy, decision trees can also provide fast--i.e., $O(\text{log}(n))$ query times, provided that the tree is sufficiently balanced. 
In this work we implement the ID3 algorithm for inducing decision trees from training data, using information gain of data features as our splitting heuristic. 
We test our implementation for correctness against two datasets from the UCI machine learning repository: the Auto-MPG dataset and the Wisconsin Breast Cancer dataset.
Constructing the decision tree classifiers for both of these datasets required not only correct implementation of the base algorithm, but also implementation of techniques for discretizing or ``binning" continuous-valued features.
In addition to correctness, we provide a comparative study of performance. 
We implement the decision tree classifier in two ways and compare those implementations gainst each other, and against the reference implementation in scikit-learn. 

\section{Implementation}
We implemented a system in Python to ingest data in \texttt{.csv} form and construct a decision tree from that data using the ID3 algorithm.
Additionally, modules for discretization of non-categorical features, pruning, k-fold cross validation, and visualization of the decision tree were implemented.
Though we always use the ID3 algorithm, we experimented with two distinct ways of representing the decision tree--in one case an object-oriented approach with explicit node objects, and in the other as a set of nested dictionaries--in order to compare their relative performance.  

\subsection{The ID3 Algorithm}
The goal of the ID3 algorithm is to construct a decision tree classifier. 
To that end, the algorithm considers subsets of the training data--initially the entire set--and selects a feature on which to partition the subset. 
Selection of a feature is equivalent to creation of a node in the tree with descendent edges for each value the feature can assume.
The algorithm then recurses on each branch; considering the subset of the training data that matches the value on the branch and excluding the previous feature from consideration.
Below, we provide pseudocode that explicitly describes the ID3 algorithm for building a decision tree:

\begin{algorithm}
    \caption{ID3}
    \label{alg-id3}
    \begin{algorithmic}[1]
        \Function{ID3}{training\_data}
            \If{all training data has same class}
                \State return a leaf node labeled with that class
            \ElsIf{there are no remaining features}
                \State return a leaf node labeled with the majority class
            \Else
                \State Determine the feature $f$ with maximum information gain
                \State Generate a node labeled with that feature
                \State Add an outgoing branch from that node for each value that feature can take
                \For{each branch}
                    \State Select subset of training data matching feature value of branch
                    \State Remove $f$ from features under consideration
                    \State Recurse on that subset   
                \EndFor
            \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{Design and Use}

\section{Results I: Auto-MPG Data}
The Auto-MPG dataset consists of 42 training instances, each of which has seven features: mpg, cylinders, displacement, horsepower, weight, acceleration, and model year. 
With the exception of mpg and cylinders, which are categorical, the rest of the features assume values from a continuum and therefore must be discretized prior to classification. 
We implement and test two discretization strategies and present accuracy results for each.

\subsection{Discretization of Continuous Values}

\subsection{Accuracy Evaluation}
Recall on training set for discretization strategy 1
Recall on test set for discretization strategy 1
Recall on training set for discretization strategy 2
Recall on test set for discretization strategy 2

\section{Results II: Wisconsin Breast Cancer Data}
The Wisconsin Breast Cancer dataset is considerably larger than the Auto-MPG dataset, consisting of 419 training instances and 140 test instances, each of which has nine features. 
The features are: clump thickness, uniform cell size, uniform cell shape, marginal adhesion, single epithelial cell size, bare nuclei, bland chromatin, normal nucleoli, and mitoses--each of which assumes integer values from 1 to 10. 

\subsection{Accuracy Evaluation}
Accuracy on training set
Accuracy on test set
Size of decision tree in terms of depth and total number of nodes

\subsection{Effect of Pruning}
Pruning a decision tree classifier can be helpful when the model has suffered from overfitting--i.e., it has learned not only the underlying function that maps vectors of features to classes, but also the inherent noise in the training set. 
We propose pruning the decision tree in the following way. 
For each non-root and non-leaf node, replace the subtree rooted at that node by a leaf node labeled with the majority class in the subset of the training data associated with that node. 
Hence, for each such interior node, we generate a new decision tree classifier.
By testing each of those new decision trees against a \emph{validation set} set aside fromboth the training set and testing set, we can evaluate the fitness of each of the pruned trees. 
We would then take the pruned tree that performed best against the validation set and test it against the test set. 

\section{Extensions}

\subsection{Performance Comparison}

\subsection{K-fold Cross Validation}

\subsection{Comparison with scikit-learn}

\section{Conclusions}

%\bibliographystyle{IEEEtran}
%\bibliography{bibliography}

\end{document}
